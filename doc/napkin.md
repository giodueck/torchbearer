# Quick initial ideas and design goals

## Main design goals
- Batchable: ability to set up jobs to be run one after the other with one command, so they can run at night and unsupervised.
- Good metrics: each job records and saves data about training and validation performance.
- Model and job description scheme: jobs need to describe the models they want and what parameters to run the training at.
- Flexible training goals: ability to limit a training run based on epochs, overfitting and maybe performance targets.
- Extensible: make dataloader pluggable. Make POC with the surface water example, but implement the main logic myself. Eventually make my own dataloader with the surface water sample data.
- Good: this is Python, but hopefully good Python. Avoid technical debt and engineer effortfully from the start.

## Implementation ideas
Job manager/dispatcher in some other language that is nicer to develop in (maybe Zig?). Since the Python parts would be pretty much the same for every job, the job manager could just call that and handle configuration and dispatching.

## Crazy/fringe ideas
Use libtorch (C++) to build the training and stuff without Python? Would have to have some performance benefits for the extra effort though.

# Architectural ideas

## Job environment
Each run gets an environment to put its stuff in: downloads are put in a central downloads directory, but model weights, temp files and other data is put in this directory.

stdout and stderr captured into files:
- by command (bash style >stdout.out 2>stderr.out) or,
- by pipe (handling output and formatting it nicely to be viewed later)

Do the first option first as it is faster.

## Job description
Model description and training code and stuff will still be done in Pytorch, no bindings or stuff like that.

Jobs will be able to take parameters in through either a file or arguments and run from there, and the torchbearer will manage these parameters and launch training jobs.

What would be interesting is if somehow the pytorch code could be made modular, so training tasks could take in the model to use as one of the parameters.
Do this in a second step though.

## Error handling
Failures need to be handled gracefully, logged and stored. Training of subsequent models must go on without failure.

## Simplification
Use Pytorch Lightning to simplify training and enable checkpoints and early stopping.

## Early stopping and validation loss
Validation loss instead of training loss to detect overfitting.

Patience: if validation loss does not improve for N epochs, stop the training.

Save the best model based on validation performance, not just the latest.

## Stack
### Option 1:
- Kubernetes to manage jobs and persistent storage
- Docker image to host pytorch
- New jobs added manually, through the latest docker image, or generated by some script

- +Learn existing tools only
- +Integrate into existing infrastructure if already set up
- -Many dependencies: to create a job queue we need redis or celery, apparently

### Option 2:
- Docker image to host pytorch
- Custom program to manage jobs, launching docker and managing persistent storage
- New jobs added manually, through the latest docker image, or generated by some script

- +Simple approach: simple tool with functionality limited to my needs
- +Complete control: logs as I want them, control of running time inside the program
- -More work, possibly too complicated for the needs

# Tools

- Pytorch
- Pytorch Lightning
- Docker
- Kubernetes?
- Visualizations:
    - Grafana
    - Tensorboard
    - Matplot?
